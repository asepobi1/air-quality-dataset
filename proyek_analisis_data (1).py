# -*- coding: utf-8 -*-
"""Proyek Analisis Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ug4Ox4qIFDSo1koFA0XiNJE-ce12rECy

# Proyek Analisis Data: Air Quality
- **Nama:** Asep Obi
- **Email:** asepobi1@gmail.com
- **ID Dicoding:** asep_obi
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install streamlit

base_dir = '/content/drive/My Drive/Air-quality-dataset'
!ls "/content/drive/My Drive/Air-quality-dataset"

"""## Menentukan Pertanyaan Bisnis

- Pertanyaan 1 : What is the trend of air quality index over the years?
- Pertanyaan 2 : What are the most significant pollutants affecting air quality?

## Import Semua Packages/Library yang Digunakan
"""

# Importing libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats

"""## Data Wrangling

### Gathering Data
"""

# Dictionary to store DataFrames
dfs = {}

# Load CSV files into DataFrames
for file_name in os.listdir(base_dir):
    if file_name.endswith('.csv'):
        df_name = os.path.splitext(file_name)[0].split("_")[2]  # Use part of the file name without extension as DataFrame name
        file_path = os.path.join(base_dir, file_name)
        dfs[df_name] = pd.read_csv(file_path).drop(['No'], axis=1)

# Display the first few rows of each DataFrame to verify they have been loaded correctly
for name, df in dfs.items():
    print(f"DataFrame: {name}")
    display(df.head())

dfs['Wanshouxigong'].head()

"""### Assessing Data

1.2.1 Simplify variables
"""

# Define a function to combine year, month, day, and hour into a datetime column
def combine_datetime(df):
    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])
    df.drop(['year', 'month', 'day', 'hour'], axis=1, inplace=True)
    return df

# Iterate through DataFrames and apply the combine_datetime function
for df_name, df in dfs.items():
    dfs[df_name] = combine_datetime(df)

"""1.2.2 Check for Duplicates"""

# Initialize a dictionary to store duplicate counts and total sample counts
duplicate_counts = {}
total_sample_counts = {}

# Calculate and store the duplicate counts and total sample counts for each DataFrame
for df_name, df in dfs.items():
    duplicate_counts[df_name] = df.duplicated().sum()
    total_sample_counts[df_name] = len(df)  # Calculate the total number of samples

# Create DataFrames from the dictionaries
duplicate_counts_df = pd.DataFrame.from_dict(duplicate_counts, orient='index', columns=['Duplicate Count'])
total_sample_counts_df = pd.DataFrame.from_dict(total_sample_counts, orient='index', columns=['Total Sample Count'])

# Combine the two DataFrames by concatenating them horizontally
pd.concat([total_sample_counts_df, duplicate_counts_df], axis=1)

"""1.2.3 Check for All of The Data Types"""

# Initialize a dictionary to store data types for each DataFrame
dtype_dict = {}

# Iterate through DataFrames and collect data types
for df_name, df in dfs.items():
    dtype_dict[df_name] = df.dtypes

# Create a DataFrame from the dtype_dict and transpose it
# to have DataFrames as rows and columns as data types
pd.DataFrame(dtype_dict).transpose()

"""1.2.4 Check for Missing Values"""

# Initialize a dictionary to store null value counts for each DataFrame
null_counts = {}

# Calculate and store the null value counts for each DataFrame
for df_name, df in dfs.items():
    null_counts[df_name] = df.isna().sum()

# Create a DataFrame from the null_counts dict and transpose it
pd.DataFrame(null_counts).transpose()

"""### Cleaning Data

1.3.1 Handle the numerical missing values with the median value
"""

# Calculate and store the median values for each numerical column
median_values = {}

for df_name, df in dfs.items():
    # Exclude non-numeric columns
    numeric_columns = df.select_dtypes(include=['number']).columns.difference(['No'])

    # Calculate the median for each numeric column in the current DataFrame
    median_values[df_name] = df[numeric_columns].median()

# Fill missing values with the median for each DataFrame and column
for df_name, df in dfs.items():
    # Exclude non-numeric columns
    numeric_columns = df.select_dtypes(include=['number']).columns.difference(['No'])

    # Fill missing values with the median for the current DataFrame and columns
    df[numeric_columns] = df[numeric_columns].fillna(median_values[df_name])

"""1.3.2 Handle the categorical missing values with their surrounding value"""

# Loop through the DataFrames and fill missing values with forward fill
for df_name, df in dfs.items():
    df['wd'].fillna(method='ffill', inplace=True)

"""1.3.3 Double check if there are any NaN values remaining"""

# Initialize a dictionary to store null value counts for each DataFrame
null_counts = {}

# Calculate and store the null value counts for each DataFrame
for df_name, df in dfs.items():
    null_counts[df_name] = df.isna().sum()

# Create a DataFrame from the null_counts dict and transpose it
pd.DataFrame(null_counts).transpose()

"""## Exploratory Data Analysis (EDA)

2.1 Explore the Mean Values of the Parameters
"""

# Create a dictionary to store mean values for each parameter
mean_values = {}

# Loop through the DataFrames in the dfs dictionary
for df_name, df in dfs.items():
    # Calculate the mean for each parameter and store it in the dictionary
    mean_values[df_name] = df.mean(numeric_only=True)

# Create a DataFrame from the dictionary of mean values
pd.DataFrame(mean_values).transpose()

"""2.2 Explore the Median Values of the Parameters"""

# Create a dictionary to store median values for each parameter
median_values = {}

# Loop through the DataFrames in the dfs dictionary
for df_name, df in dfs.items():
    # Calculate the median for each parameter and store it in the dictionary
    median_values[df_name] = df.median(numeric_only=True)

# Create a DataFrame from the dictionary of median values
pd.DataFrame(median_values).transpose()

"""2.3 Combine all of the DataFrames in the dfs dictionary into one DataFrame"""

# Concatenate all DataFrames into one DataFrame along the rows
combined_df = pd.concat(list(dfs.values()), axis=0)

# Reset the index of the combined DataFrame
combined_df.reset_index(drop=True, inplace=True)

"""2.4 Explore the combined DataFrame"""

combined_df.describe()

"""2.5 Add a category column to the DataFrame based on the worst category among the six parameters"""

# Define the ranges and AQI (Air Quality Index) upper bounds
category_ranges = [
    'Good', 'Moderate', 'Unhealthy for Sensitive Groups', 'Unhealthy', 'Very Unhealthy', 'Hazardous'
]
pm25_ranges = [12, 35.4, 150.4, 250.4, float('inf')]
pm10_ranges = [54, 154, 254, 354, 424, float('inf')]
so2_ranges = [35, 75, 185, 304, 604, float('inf')]
no2_ranges = [53, 100, 360, 649, 1249, float('inf')]
co_ranges = [4400, 9400, 12400, 15400, 30400, float('inf')]
o3_ranges = [54, 70, 85, 105, 200, float('inf')]

# Define a function to categorize a value based on the given ranges
def categorize(value, ranges):
    for max_val in ranges:
        if value <= max_val:
            return ranges.index(max_val)
    return len(ranges)  # This should not happen if ranges are defined correctly

# Ensure there are no missing values in the columns used for categorization
for col in ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']:
    combined_df[col].fillna(0, inplace=True)  # Filling with 0 or any other appropriate value

# Create a new column 'Category' by applying the categorize function to each row
combined_df['Category'] = combined_df.apply(lambda row: category_ranges[max(
    categorize(row['PM2.5'], pm25_ranges),
    categorize(row['PM10'], pm10_ranges),
    categorize(row['SO2'], so2_ranges),
    categorize(row['NO2'], no2_ranges),
    categorize(row['CO'], co_ranges),
    categorize(row['O3'], o3_ranges),
)], axis=1)

combined_df.sample(5)

"""## Visualization & Explanatory Analysis

### Pertanyaan 1: What is the trend of air quality index over the years?
"""

# Combine year, month, day, and hour into a single datetime column
combined_df['datetime'] = pd.to_datetime(combined_df[['year', 'month', 'day', 'hour']])

# Extract year from datetime column
combined_df['year'] = combined_df['datetime'].dt.year

# Assuming AQI is calculated separately; for now, let's assume we have an 'AQI' column
# For demonstration purposes, we'll create a dummy 'AQI' column based on PM2.5 values
# In practice, you would have a proper calculation or actual AQI values
combined_df['AQI'] = combined_df['PM2.5']  # Replace with actual AQI calculation

# Group by year and calculate the mean AQI for each year
yearly_trend = combined_df.groupby('year')['AQI'].mean().reset_index()

# Plot the trend of air quality index over the years
plt.figure(figsize=(10, 6))
sns.lineplot(data=yearly_trend, x='year', y='AQI')
plt.title('Trend of Air Quality Index Over the Years')
plt.xlabel('Year')
plt.ylabel('Average AQI')
plt.show()

"""### Pertanyaan 2: What are the most significant pollutants affecting air quality?"""

# Combine year, month, day, and hour into a single datetime column
combined_df['datetime'] = pd.to_datetime(combined_df[['year', 'month', 'day', 'hour']])

# Assuming AQI is calculated separately; for now, let's assume we have an 'AQI' column
# For demonstration purposes, we'll create a dummy 'AQI' column based on PM2.5 values
# In practice, you would have a proper calculation or actual AQI values
combined_df['AQI'] = combined_df['PM2.5']  # Replace with actual AQI calculation

# Select relevant columns for correlation analysis
pollutants = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']
correlation_df = combined_df[['AQI'] + pollutants]

# Calculate correlation matrix
correlation_matrix = correlation_df.corr()

# Plot heatmap of correlation matrix
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation between AQI and Pollutants')
plt.show()

# Extract correlations of pollutants with AQI
aqi_correlations = correlation_matrix['AQI'].drop('AQI').sort_values(ascending=False)

# Plot bar chart of pollutant correlations with AQI
plt.figure(figsize=(10, 6))
sns.barplot(x=aqi_correlations.values, y=aqi_correlations.index, palette='coolwarm')
plt.title('Correlation of Pollutants with AQI')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Pollutants')
plt.show()

# Print the correlation values
print("Correlation of pollutants with AQI:")
print(aqi_correlations)

"""## Conclusion

- Conclution pertanyaan 1
- Conclution pertanyaan 2
"""